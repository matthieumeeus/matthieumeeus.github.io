---
---

@article{meeus2025canary,
      title={The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text}, 
      author={Matthieu Meeus and Lukas Wutschitz and Santiago Zanella-Béguelin and Shruti Tople and Reza Shokri},
      journal={arXiv preprint arXiv:2502.14921},
      abbr={Preprint},
      year={2025},
      abstract={TLDR; We propose the first privacy auditing pipeline for synthetic text. 
                We implement different MIAs just based on access to the text and
                find that canaries with low-perplexity-prefix and high-perplexity-suffix are the most vulnerable.},
      link={https://arxiv.org/abs/2502.14921},
      press={Work done during my internship at Microsoft Research.}
}

@article{meeus2024chocollama,
  title={ChocoLlama: Lessons Learned From Teaching Llamas Dutch},
  author={Meeus, Matthieu and Rath{\'e}, Anthony and Remy, Fran{\c{c}}ois and Delobelle, Pieter and Decorte, Jens-Joris and Demeester, Thomas},
  journal={arXiv preprint arXiv:2412.07633},
  abbr={Preprint},
  year={2024},
  selected={true},
  abstract={TLDR; We further pretrain Llama-2/3 on Dutch data, and release a family of 6 open-source LLMs. 
            We elaborate on our learnings in the paper (modifying the tokenizer, 
            using LoRA at scale for language adaptation, pretraining versus posttraining, benchmarking).},
  link={https://arxiv.org/pdf/2412.07633},
  code={https://github.com/ChocoLlamaModel/ChocoLlama},
  press={All 6 models are available on [HuggingFace](https://huggingface.co/ChocoLlama). Press coverage in Flemish newspaper [De Tijd](https://www.tijd.be/ondernemen/technologie/computerwetenschappers-bouwen-vlaams-ai-model-chocollama/10585956.html).}
}

@article{meeus2024inherent,
  title={SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)},
  author={Meeus, Matthieu and Igor, Shilov and Jain, Shubham and Faysse, Manuel and Rei, Marek and de Montjoye, Yves-Alexandre},
  journal={IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2025)},
  abbr={SaTML},
  year={2025},
  abstract={TLDR; We wrote an SoK on recent developments in MIAs against LLMs.  We discuss how things have evolved recently, show popular evaluation setups to be flawed, and examine solutions going forward.},
  link={https://arxiv.org/pdf/2406.17975},
  code={https://github.com/computationalprivacy/mia_llms_benchmark},
  press={Received Best Paper Award at SaTML 2025.}
}

@article{shilov2024mosaic,
  title={Mosaic Memory: Fuzzy Duplication in Copyright Traps for Large Language Models},
  author={Shilov, Igor and Meeus, Matthieu and de Montjoye, Yves-Alexandre},
  journal={arXiv preprint arXiv:2405.15523},
  abbr={Preprint},
  year={2024},
  link={https://arxiv.org/pdf/2405.15523}
}

@article{guepin2024lost,
  title={Lost in the Averages: A New Specific Setup to Evaluate Membership Inference Attacks Against Machine Learning Models},
  author={Gu{\'e}pin, Florent and Kr{\v{c}}o, Nata{\v{s}}a and Meeus, Matthieu and de Montjoye, Yves-Alexandre},
  journal={arXiv preprint arXiv:2405.15423},
  abbr={Preprint},
  year={2024},
  link={https://arxiv.org/pdf/2405.15423}
}

@inproceedings{meeus2024did,
  title={Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models},
  author={Meeus, Matthieu and Jain, Shubham and Rei, Marek and de Montjoye, Yves-Alexandre},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  abbr={USENIX Security},
  pages={2369--2385},
  year={2024},
  selected={true},
  abstract={TLDR; Given a pretrained LLM and a document, can I infer whether the document was used to train the LLM?
            First, we rely on the collection of documents which we know have been used to train the LLM (members) and documents made available after the model release data (non-members).
            We then query the LLM on both members and non-members for token-level probabilities and train a classifier to predict binary membership.
            Spoiler: It's harder than you think!},
  link={https://arxiv.org/pdf/2310.15007},
  code={https://github.com/computationalprivacy/document-level-membership-inference},
  press={Press coverage in [Le Monde](https://www.lemonde.fr/sciences/article/2023/11/16/comment-savoir-si-un-contenu-a-ete-utilise-par-une-intelligence-artificielle_6200425_1650684.html).}
}

@inproceedings{meeuscopyright,
  title={Copyright Traps for Large Language Models},
  author={Meeus, Matthieu and Shilov, Igor and Faysse, Manuel and de Montjoye, Yves-Alexandre},
  booktitle={Forty-first International Conference on Machine Learning},
  abbr={ICML},
  year={2024},
  selected={true},
  abstract={TLDR; We add copyright traps to original content.
        These are highly unique sequences that, if an LLM were to be trained on it, we would be able to tell through how the LLM reacts to our injected trap.
        We inject a variety of traps into the pretraining dataset of the real-world 1.3B CroissantLLM trained from scratch, and find that copyright traps indeed enable content detectability.},
  link={https://arxiv.org/pdf/2402.09363},
  code={https://github.com/computationalprivacy/copyright-traps},
  press={Press coverage in [MIT Technology Review](https://www.technologyreview.com/2024/07/25/1095347/a-new-tool-for-copyright-holders-can-show-if-their-work-is-in-ai-training-data/) and [Nature News](https://www.nature.com/articles/d41586-024-02599-9).}
}

@inproceedings{meeus2023achilles,
  title={Achilles’ Heels: Vulnerable Record Identification in Synthetic Data Publishing},
  author={Meeus, Matthieu and Guepin, Florent and Cre{\c{t}}u, Ana-Maria and de Montjoye, Yves-Alexandre},
  booktitle={European Symposium on Research in Computer Security},
  abbr={ESORICS},
  pages={380--399},
  year={2023},
  organization={Springer},
  selected={true},
  abstract={TLDR; We audit the privacy risk of synthetic tabular data through Membership Inference Attacks (MIAs).
        For this, we are most concerned about the worst-case risk - so we propose a method to identify the most at-risk data records in a dataset.
        We show that our vulnerable record identification method beats previously used, ad-hoc outlier detection mechanisms significantly.},
  link={https://arxiv.org/pdf/2306.10308},
  code={https://github.com/computationalprivacy/MIA-synthetic}
}

@inproceedings{guepin2023synthetic,
  title={Synthetic Is All You Need: Removing the Auxiliary Data Assumption for Membership Inference Attacks Against Synthetic Data},
  author={Gu{\'e}pin, Florent and Meeus, Matthieu and Cre{\c{t}}u, Ana-Maria and de Montjoye, Yves-Alexandre},
  booktitle={European Symposium on Research in Computer Security},
  pages={182--198},
  year={2023},
  abbr={ESORICS Workshop},
  organization={Springer},
  abstract={TLDR; In Membership Inference Attacks (MIAs) against synthetic data, we typically assume the attacker to have access to some auxiliary data (from the same distribution as the real training data).
        In practice, this is not that realistic, especially for use cases typically suggested for synthetic data.
        We here examine what happens to the MIA performance when we use the released synthetic itself as a replacement for the auxiliary dataset to build shadow-modeling based MIAs.
        Spoiler: MIAs still work, but with a substantial drop compared to real auxiliary data. },
  link={https://arxiv.org/pdf/2307.01701},
  code={https://github.com/computationalprivacy/MIA-synthetic}
}


@article{meeus2023concerns,
  title={Concerns about using a digital mask to safeguard patient privacy},
  author={Meeus, Matthieu and Jain, Shubham and de Montjoye, Yves-Alexandre},
  journal={Nature Medicine},
  volume={29},
  number={7},
  pages={1658--1659},
  year={2023},
  publisher={Nature Publishing Group US New York},
  selected={true},
  abbr={Nature Medicine},
  abstract={TLDR; A widely covered Nature paper introduces a Digital Mask (DM), an 'anonymization' algorithm to be applied to facial images of patients.
  Reportedly, the mask would irreversibly erase all identifiable features while retaining the information necessary for medical diagnosis.
  We show their setup to evaluate the anonymization provided by the DM to be seriously flawed, and show that in a proper setup, the risk of identification increases by 100X.},
  link={https://www.nature.com/articles/s41591-023-02439-9},
  code={https://github.com/computationalprivacy/unmask}
}