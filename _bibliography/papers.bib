---
---

@inproceedings{meeus2024did,
  title={Did the Neurons Read your Book? Document-level Membership Inference for Large Language Models},
  author={Meeus, Matthieu and Jain, Shubham and Rei, Marek and de Montjoye, Yves-Alexandre},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  abbr={USENIX Security},
  pages={2369--2385},
  year={2024},
  selected={true},
  tldr={TLDR; Given a pretrained LLM and a document, can I infer whether the document was used to train the LLM?
            First, we rely on the collection of documents which we know have been used to train the LLM (members) and documents made available after the model release data (non-members).
            We then query the LLM on both members and non-members for token-level probabilities and train a classifier to predict binary membership.
            Spoiler: It's harder than you think!},
  link={https://arxiv.org/pdf/2310.15007},
  code={https://github.com/computationalprivacy/document-level-membership-inference},
  press={Press coverage in [Le Monde](https://www.lemonde.fr/sciences/article/2023/11/16/comment-savoir-si-un-contenu-a-ete-utilise-par-une-intelligence-artificielle_6200425_1650684.html).}
}

@inproceedings{meeuscopyright,
  title={Copyright Traps for Large Language Models},
  author={Meeus, Matthieu and Shilov, Igor and Faysse, Manuel and de Montjoye, Yves-Alexandre},
  booktitle={Forty-first International Conference on Machine Learning},
  abbr={ICML},
  year={2024},
  selected={true},
  tldr={TLDR; We add copyright traps to original content.
        These are highly unique sequences that, if an LLM were to be trained on it, we would be able to tell through how the LLM reacts to our injected trap.
        We inject a variety of traps into the pretraining dataset of the real-world 1.3B CroissantLLM trained from scratch, and find that copyright traps indeed enable content detectability.},
  link={https://arxiv.org/pdf/2402.09363},
  code={https://github.com/computationalprivacy/copyright-traps},
  press={Press coverage in [MIT Technology Review](https://www.technologyreview.com/2024/07/25/1095347/a-new-tool-for-copyright-holders-can-show-if-their-work-is-in-ai-training-data/) and [Nature News](https://www.nature.com/articles/d41586-024-02599-9).}
}

@inproceedings{meeus2023achilles,
  title={Achillesâ€™ Heels: Vulnerable Record Identification in Synthetic Data Publishing},
  author={Meeus, Matthieu and Guepin, Florent and Cre{\c{t}}u, Ana-Maria and de Montjoye, Yves-Alexandre},
  booktitle={European Symposium on Research in Computer Security},
  abbr={ESORICS},
  pages={380--399},
  year={2023},
  organization={Springer},
  selected={true}
  tldr={TLDR; We audit the privacy risk of synthetic tabular data through Membership Inference Attacks (MIAs).
        For this, we are most concerned about the worst-case risk - so we propose a method to identify the most at-risk data records in a dataset.
        We show that our vulnerable record identification method beats previously used, ad-hoc outlier detection mechanisms significantly.},
  link={https://arxiv.org/pdf/2306.10308},
  code={https://github.com/computationalprivacy/MIA-synthetic}
}

@inproceedings{guepin2023synthetic,
  title={Synthetic Is All You Need: Removing the Auxiliary Data Assumption for Membership Inference Attacks Against Synthetic Data},
  author={Gu{\'e}pin, Florent and Meeus, Matthieu and Cre{\c{t}}u, Ana-Maria and de Montjoye, Yves-Alexandre},
  booktitle={European Symposium on Research in Computer Security},
  pages={182--198},
  year={2023},
  abbr={ESORICS Workshop},
  organization={Springer}
}

@article{meeus2024inherent,
  title={Inherent Challenges of Post-Hoc Membership Inference for Large Language Models},
  author={Meeus, Matthieu and Jain, Shubham and Rei, Marek and de Montjoye, Yves-Alexandre},
  journal={arXiv preprint arXiv:2406.17975},
  abbr={Preprint},
  year={2024}
}

@article{shilov2024mosaic,
  title={Mosaic Memory: Fuzzy Duplication in Copyright Traps for Large Language Models},
  author={Shilov, Igor and Meeus, Matthieu and de Montjoye, Yves-Alexandre},
  journal={arXiv preprint arXiv:2405.15523},
  abbr={Preprint},
  year={2024}
}

@article{meeus2023concerns,
  title={Concerns about using a digital mask to safeguard patient privacy},
  author={Meeus, Matthieu and Jain, Shubham and de Montjoye, Yves-Alexandre},
  journal={Nature Medicine},
  volume={29},
  number={7},
  pages={1658--1659},
  year={2023},
  publisher={Nature Publishing Group US New York},
  selected={true},
  abbr={Nature Medicine},
  tldr={TLDR; A widely covered Nature paper introduces a Digital Mask (DM), an 'anonymization' algorithm to be applied to facial images of patients.
  Reportedly, the mask would irreversibly erase all identifiable features while retaining the information necessary for medical diagnosis.
  We show their setup to evaluate the anonymization provided by the DM to be seriously flawed, and show that in a proper setup, the risk of identification increases by 100X.},
  link={https://www.nature.com/articles/s41591-023-02439-9},
  code={https://github.com/computationalprivacy/unmask}
}

@article{guepin2024lost,
  title={Lost in the Averages: A New Specific Setup to Evaluate Membership Inference Attacks Against Machine Learning Models},
  author={Gu{\'e}pin, Florent and Kr{\v{c}}o, Nata{\v{s}}a and Meeus, Matthieu and de Montjoye, Yves-Alexandre},
  journal={arXiv preprint arXiv:2405.15423},
  abbr={Preprint},
  year={2024}
}